{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Based Review Responder with Flan-T5\n",
    "\n",
    "In this Notebook we want to use the output from our Sentiment Analysis model that we trained with the Yelp Dataset, to give a sentiment-based response to the reviews."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "\n",
    "For this notebook, I am only going to use the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 21:11:40.223599: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-11 21:11:44.649338: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-11 21:11:44.651424: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-11 21:11:44.652452: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model\n",
    "model_lstm = load_model('models/sentiment_analysis_model_lstm.h5')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the model, we have to initate the tokenizer and preprocess the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14673 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# If the preprocessing has already been done and you're ram is big enough, you can load the preprocessed data all at once\n",
    "# But if your ram is not big enough, you can load the data in chunks and preprocess them in chunks under \"Loading Preprocessed Data in Chunks\"\n",
    "df_preprocessed= pd.read_json('preprocessing/preprocessed_reviews_reduced_5k.json')\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 10000\n",
    "# Max number of words in each review.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Extract the strings from the dictionaries in the 'text' column\n",
    "df_preprocessed['text'] = df_preprocessed['text'].apply(lambda x: list(x.values())[0] if isinstance(x, dict) else x)\n",
    "\n",
    "# Extract the ratings from the dictionaries in the 'stars' column\n",
    "df_preprocessed['stars'] = df_preprocessed['stars'].apply(lambda x: list(x.values())[0] if isinstance(x, dict) else x)\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@\\[\\]^_`{|}~', lower=True)\n",
    "\n",
    "# Fit the tokenizer on the texts\n",
    "tokenizer.fit_on_texts(df_preprocessed['text'].values)\n",
    "\n",
    "# Vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Transform text to sequence of integers\n",
    "X = tokenizer.texts_to_sequences(df_preprocessed['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dinopelesevic/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2023-06-11 21:11:55.016036: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-11 21:11:55.017273: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-11 21:11:55.018914: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 538ms/step\n",
      "[[0.20301124 0.19715099 0.20036684 0.2017606  0.19771037]\n",
      " [0.20172176 0.19951713 0.20218927 0.2005564  0.19601545]\n",
      " [0.20104069 0.1990636  0.19819582 0.20299238 0.19870758]\n",
      " [0.20035619 0.20165557 0.19830921 0.20424369 0.19543532]\n",
      " [0.20190658 0.20102546 0.20063524 0.19954455 0.19688813]\n",
      " [0.20139937 0.19881059 0.19969276 0.20307936 0.19701792]]\n"
     ]
    }
   ],
   "source": [
    "# Download the stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    processed_reviews = []\n",
    "    for review in reviews:\n",
    "        # Convert to lowercase\n",
    "        review = review.lower()\n",
    "        # Remove punctuation\n",
    "        review = re.sub(r'[^\\w\\s]', '', review)\n",
    "        # Remove stopwords and stem the words\n",
    "        review = ' '.join(stemmer.stem(word) for word in review.split() if word not in stop_words)\n",
    "        processed_reviews.append(review)\n",
    "    return processed_reviews\n",
    "\n",
    "# Select a few reviews to test the model\n",
    "test_reviews = [\n",
    "    'The food was absolutely wonderful, from preparation to presentation, very pleasing.',\n",
    "    'The staff did not give us good service.',\n",
    "    'The restaurant was not clean. Our food was terrible.',\n",
    "    'The food was delicious and the service was great!',\n",
    "    'The food was ok. But we liked the service.',\n",
    "    'We ate not fine, but the food was not great at all!'\n",
    "]\n",
    "\n",
    "# Preprocess the test reviews\n",
    "test_reviews = preprocess_reviews(test_reviews)\n",
    "\n",
    "# Convert the test reviews into sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_reviews)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "# Make predictions on the test reviews\n",
    "predictions_lstm = model_lstm.predict(test_sequences)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Responder with Flan-T5\n",
    "\n",
    "Based on the sentiment of the review, we will generate a response using the Flan-T5 model. If the review is positive, we will generate a positive response. If the review is negative, we will generate a negative response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dinopelesevic/Documents/ML2/ML2-Project/venv/lib/python3.9/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# model = TFAutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\") # If you have enough ram, you can use the XL model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\") \n",
    "\n",
    "# Define a function that takes an input text and a star rating and generates a response\n",
    "def generate_response(input_text, stars):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Add conditioning text based on the number of stars\n",
    "    if stars == 1:\n",
    "        prefix = \"You're a restaurant owner and got the following review. Give answer and apologize:\"\n",
    "    elif stars == 2:\n",
    "        prefix = \"You're a restaurant owner and got the following review. Give answer and apologize:\"\n",
    "    elif stars == 3:\n",
    "        prefix = \"You're a restaurant owner and got the following review:\"\n",
    "    elif stars == 4:\n",
    "        prefix = \"You're a restaurant owner and got the following review. Give a thankful answer:\"\n",
    "    elif stars == 5:\n",
    "        prefix = \"You're a restaurant owner and got the following review. You're very happy and want to thank the customer:\"\n",
    "\n",
    "    input_text = f\"{prefix} {input_text}\"\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors=\"tf\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        min_length=0,\n",
    "        max_length=512,  # Adjust as needed\n",
    "        length_penalty=2.0,\n",
    "        num_beams=16,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    print(f\"Output: {output_text[0]}, Rating: {stars}\")\n",
    "    print(f\"Inference time: {total_time} seconds\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Now we can test the sentiment-based review-response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: This is a great place to eat!, Rating: 3\n",
      "Inference time: 3.8322629928588867 seconds\n",
      "Output: The staff did not give us good service, Rating: 1\n",
      "Inference time: 3.2041401863098145 seconds\n",
      "Output: The food was mediocre., Rating: 5\n",
      "Inference time: 3.346299171447754 seconds\n",
      "Output: This is a great restaurant!, Rating: 4\n",
      "Inference time: 3.4511916637420654 seconds\n",
      "Output: The food was okay. But we liked the service., Rating: 2\n",
      "Inference time: 4.715893983840942 seconds\n",
      "Output: The food was not great at all, Rating: 5\n",
      "Inference time: 3.043585777282715 seconds\n"
     ]
    }
   ],
   "source": [
    "# Select a few reviews to test the model\n",
    "reviews = [\n",
    "    'The food was absolutely wonderful, from preparation to presentation, very pleasing.',\n",
    "    'The staff did not give us good service.',\n",
    "    'The restaurant was not clean. Our food was terrible.',\n",
    "    'The food was delicious and the service was great!',\n",
    "    'The food was ok. But we liked the service.',\n",
    "    'We ate not fine, but the food was not great at all!'\n",
    "]\n",
    "\n",
    "# Print the predictions\n",
    "for i, review in enumerate(reviews):\n",
    "    generate_response(review, predictions_lstm[i])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "\n",
    "The answers are generated and the code works, but the quality of the answers is very insufficient. This is due to the Flan-T5-Small model, which is very small.\n",
    "\n",
    "The Flan-T5-XXL model is much better trained and has a better quality in the generation of text. Attached are two screenshots of the Huggingface hosted API:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1-StarReview](../images/1-StarReview.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5-Star Review](../images/5-StarReview.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
