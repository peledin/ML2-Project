{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "This notebook presents an exploration into predicting the star rating of a restaurant review using different types of machine learning models - Convolutional Neural Network (CNN), Long Short Term Memory (LSTM), and Bidirectional Encoder Representations from Transformers (BERT).\n",
    "\n",
    "All the models aim to predict the star rating of a restaurant review based on the text of the review. The models are trained on the Yelp Dataset from Kaggle: https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset?select=yelp_academic_dataset_review.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "In this first cell, all necessary libraries for this notebook are imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Preprocessing involves cleaning the text data, removing unwanted words, converting it into a form that is predictive. This process involves several steps:\n",
    "\n",
    "1. Downloading the NLTK stopwords: Stopwords are words that you want to ignore, so you download a list of these words from NLTK, which is a toolkit for natural language processing.\n",
    "2. The function preprocess_text() is defined to convert the text into lower case, remove punctuation, remove stopwords, and stem the words.\n",
    "3. The data is loaded and processed in chunks to reduce RAM usage. I had to this here because we're dealing with large amounts of data that did not fit into memory.\n",
    "\n",
    "The cleaned and preprocessed data is then saved for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dinopelesevic/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dinopelesevic/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed chunk 0\n",
      "preprocessed chunk 1\n",
      "preprocessed chunk 2\n",
      "preprocessed chunk 3\n",
      "preprocessed chunk 4\n",
      "Number of rows in the DataFrame: 5000\n"
     ]
    }
   ],
   "source": [
    "# Download the NLTK stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Define a function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove stopwords and stem the words\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "# Define the chunk size\n",
    "chunksize = 1000 # for 5k reviews\n",
    "# chunksize = 10000 # for 100k reviews\n",
    "# chunksize = 50000 # for 1M reviews\n",
    "# chunksize = 100000 # for the entire dataset\n",
    "\n",
    "# Initialize an empty DataFrame to store the preprocessed data\n",
    "df_preprocessed = pd.DataFrame()\n",
    "\n",
    "# Load and preprocess the data in chunks\n",
    "for i, chunk in enumerate(pd.read_json('data/yelp_academic_dataset_review_reduced_5k.json', lines=True, chunksize=chunksize)): # for 5k reviews \n",
    "# for i, chunk in enumerate(pd.read_json('data/yelp_academic_dataset_review_reduced_100k.json', lines=True, chunksize=chunksize)): # for 100k reviews\n",
    "# for i, chunk in enumerate(pd.read_json('data/yelp_academic_dataset_review_reduced_1M.json', lines=True, chunksize=chunksize)): # for 1M reviews\n",
    "# for i, chunk in enumerate(pd.read_json('data/yelp_academic_dataset_review.json', lines=True, chunksize=chunksize)): # for the entire dataset\n",
    "    # Preprocess the text in the chunk\n",
    "    chunk['text'] = chunk['text'].apply(preprocess_text)\n",
    "    # Append the preprocessed chunk to the DataFrame\n",
    "    df_preprocessed = pd.concat([df_preprocessed, chunk])\n",
    "    # Save the preprocessed chunk to a separate file\n",
    "    chunk.to_json(f'preprocessing/preprocessed_reviews_chunk_{i}.json')\n",
    "    # Print the progress \n",
    "    print(f'preprocessed chunk {i}')\n",
    "\n",
    "# Save the entire preprocessed DataFrame\n",
    "df_preprocessed.to_json('preprocessing/preprocessed_reviews.json')\n",
    "\n",
    "# Display the first few rows of the preprocessed DataFrame\n",
    "df_preprocessed.head()\n",
    "\n",
    "# Print the number of rows in the DataFrame\n",
    "print('Number of rows in the DataFrame:', len(df_preprocessed))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Preprocessed Data in Chunks\n",
    "\n",
    "I implemented this step as my Kernel kept crashing because my RAM wasn't big enough. With this approach I was able to continue with the preprocessing even if the kernel crashed. \n",
    "\n",
    "The preprocessed data is loaded in chunks for further usage. This reduces the RAM usage as the whole data isn't loaded into memory at once. \n",
    "However, if your RAM is big enough you can load the whole preprocessed json-file at once.\n",
    "\n",
    "The data is then divided into three subsets to be used for different models - CNN, LSTM and BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunk 0\n",
      "Loading chunk 1\n",
      "Loading chunk 2\n",
      "Loading chunk 3\n",
      "Loading chunk 4\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to store the preprocessed data\n",
    "df_preprocessed = pd.DataFrame()\n",
    "\n",
    "# Load the preprocessed data chunks into the DataFrame\n",
    "# The number of chunks may vary depending on the chunk size in the previous step\n",
    "\n",
    "for i in range(5): # for 5k reviews\n",
    "# for i in range(10): # for 100k reviews\n",
    "# for i in range(20): # for 1M reviews\n",
    "# for i in range(70): # for the entire dataset\n",
    "    print(f'Loading chunk {i}')\n",
    "    # Load the chunk\n",
    "    chunk = pd.read_json(f'preprocessing/preprocessed_reviews_chunk_{i}.json')\n",
    "\n",
    "    # Append the chunk to the DataFrame\n",
    "    df_bert = pd.concat([df_preprocessed, chunk])\n",
    "    df_cnn = df_bert.copy()\n",
    "    df_lstm = df_bert.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "This section is dedicated to building a Convolutional Neural Network (CNN) for the text classification task.\n",
    "\n",
    "- First, the necessary libraries for building a CNN model are imported from TensorFlow.\n",
    "- Tokenization: The Tokenizer utility class is used to vectorize a text corpus into a list of integers. Each integer maps to a value in a dictionary that encodes the entire corpus, with the keys in the dictionary being the vocabulary of the corpus.\n",
    "- The model is defined with an Embedding layer, Conv1D layer and Dense layers. It's then compiled with categorical crossentropy loss function and Adam optimizer.\n",
    "- The model is trained for a specified number of epochs.\n",
    "- The trained model is saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6468 unique tokens.\n",
      "Shape of data tensor: (1000, 250)\n",
      "Shape of label tensor: (1000, 5)\n",
      "(900, 250) (900, 5)\n",
      "(100, 250) (100, 5)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 250, 100)          1000000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 246, 128)          64128     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,072,709\n",
      "Trainable params: 1,072,709\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13/13 [==============================] - 2s 70ms/step - loss: 1.5274 - accuracy: 0.4074 - val_loss: 1.4763 - val_accuracy: 0.4111\n"
     ]
    }
   ],
   "source": [
    "# Load Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 10000\n",
    "# Max number of words in each review.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Extract the strings from the dictionaries in the 'text' column\n",
    "df_cnn['text'] = df_cnn['text'].apply(lambda x: list(x.values())[0] if isinstance(x, dict) else x)\n",
    "\n",
    "# Extract the ratings from the dictionaries in the 'stars' column\n",
    "df_cnn['stars'] = df_cnn['stars'].apply(lambda x: list(x.values())[0] if isinstance(x, dict) else x)\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@\\[\\]^_`{|}~', lower=True)\n",
    "\n",
    "# Fit the tokenizer on the texts\n",
    "tokenizer.fit_on_texts(df_cnn['text'].values)\n",
    "\n",
    "# Vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Transform text to sequence of integers\n",
    "X = tokenizer.texts_to_sequences(df_cnn['text'].values)\n",
    "\n",
    "# Pad sequences\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "# One-hot encode labels\n",
    "Y = pd.get_dummies(df_cnn['stars']).values\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "\n",
    "# Define the model\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model_cnn.add(Conv1D(128, 5, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(64, activation='relu'))\n",
    "model_cnn.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summarize the model\n",
    "model_cnn.summary()\n",
    "\n",
    "# Define the number of epochs and the batch size\n",
    "epochs = 1 # I did 5 epochs for the whole dataset\n",
    "batch_size = 64 # and 128 batch size\n",
    "\n",
    "# Train the model\n",
    "history_cnn = model_cnn.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "# Save the model\n",
    "model_cnn.save('models/sentiment_analysis_model_cnn_5k.h5')\n",
    "# model_cnn.save('models/sentiment_analysis_model_cnn_100k.h5')\n",
    "# model_cnn.save('models/sentiment_analysis_model_cnn_1M.h5')\n",
    "# model_cnn.save('models/sentiment_analysis_model_cnn.h5')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Result after Training the CNN Model in 5 Epochs with the full Dataset](images/CNN_FullDataset_5Epochs_128Batches.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "This section is dedicated to building a Long Short Term Memory (LSTM) model for the text classification task.\n",
    "\n",
    "- Similar steps as in the CNN section are followed to build and train an LSTM model, except that the architecture of the model is different. LSTM layers are used instead of Conv1D layers.\n",
    "- The trained model is saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6468 unique tokens.\n",
      "Shape of data tensor: (1000, 250)\n",
      "Shape of label tensor: (1000, 5)\n",
      "(900, 250) (900, 5)\n",
      "(100, 250) (100, 5)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 250, 100)          1000000   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 250, 100)         0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 505       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,080,905\n",
      "Trainable params: 1,080,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 14:55:05.612304: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-11 14:55:05.613770: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-11 14:55:05.615621: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-11 14:55:05.971595: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-11 14:55:05.975526: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-11 14:55:05.977720: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-11 14:55:06.687735: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-11 14:55:06.689004: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-11 14:55:06.691493: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - ETA: 0s - loss: 1.5410 - accuracy: 0.3963"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 14:55:10.901892: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-11 14:55:10.903497: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-11 14:55:10.905247: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 6s 270ms/step - loss: 1.5410 - accuracy: 0.3963 - val_loss: 1.4550 - val_accuracy: 0.4111\n"
     ]
    }
   ],
   "source": [
    "# Load Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 10000\n",
    "# Max number of words in each review.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Extract the strings from the dictionaries in the 'text' column\n",
    "df_lstm['text'] = df_lstm['text'].apply(lambda x: list(x.values())[0] if isinstance(x, dict) else x)\n",
    "\n",
    "# Extract the ratings from the dictionaries in the 'stars' column\n",
    "df_lstm['stars'] = df_lstm['stars'].apply(lambda x: list(x.values())[0] if isinstance(x, dict) else x)\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@\\[\\]^_`{|}~', lower=True)\n",
    "\n",
    "# Fit the tokenizer on the texts\n",
    "tokenizer.fit_on_texts(df_lstm['text'].values)\n",
    "\n",
    "# Vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Transform text to sequence of integers\n",
    "X = tokenizer.texts_to_sequences(df_lstm['text'].values)\n",
    "\n",
    "# Pad sequences\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "# One-hot encode labels\n",
    "Y = pd.get_dummies(df_lstm['stars']).values\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "\n",
    "# Define the model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model_lstm.add(SpatialDropout1D(0.2))\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0))\n",
    "model_lstm.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summarize the model\n",
    "model_lstm.summary()\n",
    "\n",
    "# Define the number of epochs and the batch size\n",
    "epochs = 1\n",
    "batch_size = 64\n",
    "\n",
    "# Train the model\n",
    "history = model_lstm.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "# Save the model\n",
    "model_lstm.save('models/sentiment_analysis_model_lstm_5k.h5')\n",
    "# model_lstm.save('models/sentiment_analysis_model_lstm_100k.h5')\n",
    "# model_lstm.save('models/sentiment_analysis_model_lstm_1M.h5')\n",
    "# model_lstm.save('models/sentiment_analysis_model_lstm.h5')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Result after Training the LSTM Model in 5 Epochs with the full Dataset](images/LSTM_FullDataset_5Epochs_128Batches.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "This section covers the usage of BERT (Bidirectional Encoder Representations from Transformers) model. BERT has been pre-trained on a large corpus of text and can generate high-quality embeddings for text classification tasks.\n",
    "\n",
    "- First, the necessary libraries are imported, including the AutoTokenizer and TFDistilBertForSequenceClassification classes from the transformers library.\n",
    "\n",
    "- The BERT tokenizer is loaded using the \"distilbert-base-uncased\" model, which is a version of BERT that is smaller, faster, cheaper, and lighter.\n",
    "\n",
    "- The 'text' and 'stars' columns of the DataFrame are processed to extract the actual strings from the dictionaries.\n",
    "\n",
    "- The ratings are converted to integers and then encoded to integer labels using LabelEncoder.\n",
    "\n",
    "- The data is split into a training set and a test set.\n",
    "\n",
    "- The reviews are tokenized using the BERT tokenizer, padding and truncating them to a maximum length of 512 tokens.\n",
    "\n",
    "- The tokenized data and labels are converted into a TensorFlow dataset and batched.\n",
    "\n",
    "- The BERT model is initialized using the \"distilbert-base-uncased\" model and compiled with the Adam optimizer, the SparseCategoricalCrossentropy loss function, and the SparseCategoricalAccuracy metric.\n",
    "\n",
    "- The model is trained for the chosen amount of epochs and then saved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning for Demo Purposes:**\n",
    " The runtime for the Bert model takes a lot longer than LSTM and CNN. Even with the 5k Dataset, it takes about 3min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dinopelesevic/Documents/ML2/SentimentAnalysisYelpReviews/SentimentAnalysisYelpReview/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 268M/268M [00:06<00:00, 38.8MB/s] \n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-06-11 14:56:01.843369: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [800]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - ETA: 0s - loss: 1.4100 - sparse_categorical_accuracy: 0.4288 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 15:08:14.210567: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [200]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 780s 15s/step - loss: 1.4100 - sparse_categorical_accuracy: 0.4288 - val_loss: 1.3155 - val_sparse_categorical_accuracy: 0.4800\n"
     ]
    }
   ],
   "source": [
    "# Load Libraries\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, TFDistilBertForSequenceClassification\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Extract the strings from the dictionaries in the 'text' column\n",
    "df_bert['text'] = df_bert['text'].apply(lambda x: list(x.values())[0] if isinstance(x, dict) else x)\n",
    "\n",
    "# Extract the ratings from the dictionaries in the 'stars' column\n",
    "df_bert['stars'] = df_bert['stars'].apply(lambda x: list(x.values())[0] if isinstance(x, dict) else x)\n",
    "\n",
    "# Convert labels to integers\n",
    "df_bert['stars'] = df_bert['stars'].astype(int)\n",
    "\n",
    "# Initialize label encoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_bert['stars']) # Fit label encoder with the all data\n",
    "\n",
    "# Transform the labels\n",
    "df_bert['stars'] = le.transform(df_bert['stars'])\n",
    "\n",
    "# Split the data\n",
    "df_train, df_test = train_test_split(df_bert, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train_dataset = tokenizer(df_train['text'].tolist(), padding=True, truncation=True, max_length=512)\n",
    "tokenized_test_dataset = tokenizer(df_test['text'].tolist(), padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Convert the tokenized data and labels into a tensorflow dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": tokenized_train_dataset['input_ids'], \"attention_mask\": tokenized_train_dataset['attention_mask']}, df_train['stars'].values)).batch(16)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": tokenized_test_dataset['input_ids'], \"attention_mask\": tokenized_test_dataset['attention_mask']}, df_test['stars'].values)).batch(16)\n",
    "\n",
    "# Initialize the model\n",
    "model_bert = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(np.unique(df_bert['stars'])))\n",
    "\n",
    "# Compile the model\n",
    "model_bert.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "# Train the model\n",
    "model_bert.fit(train_dataset, epochs=1, validation_data=test_dataset)\n",
    "\n",
    "# Save the model\n",
    "model_bert.save_pretrained('models/sentiment_analysis_model_bert_5k')\n",
    "# model_bert.save_pretrained('models/sentiment_analysis_model_bert_100k')\n",
    "# model_bert.save_pretrained('models/sentiment_analysis_model_bert_1M')\n",
    "# model_bert.save_pretrained('models/sentiment_analysis_model_bert')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Result after Training the LSTM Model in 5 Epochs with the full Dataset](images/Bert_100k_5Epochs.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Models\n",
    "\n",
    "After saving the models, they are loaded for use. The model files are located in the 'models' directory and loaded using the load_model function from TensorFlow (for ltsm and cnn) and the from_pretrained method from transformers (for bert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 15:09:48.011085: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-11 15:09:48.013698: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-11 15:09:48.015274: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "Some layers from the model checkpoint at models/sentiment_analysis_model_bert_100k were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at models/sentiment_analysis_model_bert_100k and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model\n",
    "model_cnn = load_model('models/sentiment_analysis_model_cnn_5k.h5')\n",
    "model_lstm = load_model('models/sentiment_analysis_model_lstm_100k.h5')\n",
    "model_bert = TFDistilBertForSequenceClassification.from_pretrained('models/sentiment_analysis_model_bert_100k')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "The models are then tested on new data. In order to do this, the necessary libraries are imported and the preprocessing steps carried out earlier are repeated on the new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you are starting from the Loading part, you have to run this cell as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6468 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# If the preprocessing has already been done and you're ram is big enough, you can load the preprocessed data all at once\n",
    "# But if your ram is not big enough, you can load the data in chunks and preprocess them in chunks under \"Loading Preprocessed Data in Chunks\"\n",
    "df_preprocessed= pd.read_json('preprocessing/preprocessed_reviews.json')\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 10000\n",
    "# Max number of words in each review.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Extract the strings from the dictionaries in the 'text' column\n",
    "df_preprocessed['text'] = df_preprocessed['text'].apply(lambda x: list(x.values())[0] if isinstance(x, dict) else x)\n",
    "\n",
    "# Extract the ratings from the dictionaries in the 'stars' column\n",
    "df_preprocessed['stars'] = df_preprocessed['stars'].apply(lambda x: list(x.values())[0] if isinstance(x, dict) else x)\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@\\[\\]^_`{|}~', lower=True)\n",
    "\n",
    "# Fit the tokenizer on the texts\n",
    "tokenizer.fit_on_texts(df_preprocessed['text'].values)\n",
    "\n",
    "# Vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Transform text to sequence of integers\n",
    "X = tokenizer.texts_to_sequences(df_preprocessed['text'].values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of test reviews is defined, and these reviews are preprocessed and tokenized using the same steps as the training data.\n",
    "\n",
    "The models are then used to predict the ratings for these reviews, and the predicted ratings are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dinopelesevic/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 253ms/step\n",
      "1/1 [==============================] - 0s 369ms/step\n",
      "Review: food absolut wonder prepar present pleas\n",
      "Predicted rating (CNN): 5\n",
      "Predicted rating (LSTM): 5\n",
      "---\n",
      "Review: staff give us good servic\n",
      "Predicted rating (CNN): 5\n",
      "Predicted rating (LSTM): 5\n",
      "---\n",
      "Review: restaur clean food terribl\n",
      "Predicted rating (CNN): 5\n",
      "Predicted rating (LSTM): 5\n",
      "---\n",
      "Review: food delici servic great\n",
      "Predicted rating (CNN): 5\n",
      "Predicted rating (LSTM): 5\n",
      "---\n",
      "Review: food ok like servic\n",
      "Predicted rating (CNN): 5\n",
      "Predicted rating (LSTM): 5\n",
      "---\n",
      "Review: ate fine food great\n",
      "Predicted rating (CNN): 5\n",
      "Predicted rating (LSTM): 5\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 15:09:55.379213: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-11 15:09:55.382274: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-11 15:09:55.384552: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# Download the stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    processed_reviews = []\n",
    "    for review in reviews:\n",
    "        # Convert to lowercase\n",
    "        review = review.lower()\n",
    "        # Remove punctuation\n",
    "        review = re.sub(r'[^\\w\\s]', '', review)\n",
    "        # Remove stopwords and stem the words\n",
    "        review = ' '.join(stemmer.stem(word) for word in review.split() if word not in stop_words)\n",
    "        processed_reviews.append(review)\n",
    "    return processed_reviews\n",
    "\n",
    "# Select a few reviews to test the model\n",
    "test_reviews = [\n",
    "    'The food was absolutely wonderful, from preparation to presentation, very pleasing.',\n",
    "    'The staff did not give us good service.',\n",
    "    'The restaurant was not clean. Our food was terrible.',\n",
    "    'The food was delicious and the service was great!',\n",
    "    'The food was ok. But we liked the service.',\n",
    "    'We ate not fine, but the food was not great at all!'\n",
    "]\n",
    "\n",
    "# Preprocess the test reviews\n",
    "test_reviews = preprocess_reviews(test_reviews)\n",
    "\n",
    "# Convert the test reviews into sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_reviews)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "# Make predictions on the test reviews\n",
    "predictions_cnn = model_cnn.predict(test_sequences)\n",
    "predictions_lstm = model_lstm.predict(test_sequences)\n",
    "\n",
    "\n",
    "# Print the predictions\n",
    "for i, review in enumerate(test_reviews):\n",
    "    print(f'Review: {review}')\n",
    "    print(f'Predicted rating (CNN): {np.argmax(predictions_cnn[i]) + 1}')\n",
    "    print(f'Predicted rating (LSTM): {np.argmax(predictions_lstm[i]) + 1}')\n",
    "    print('---')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the BERT model testing, the DistilBertTokenizer is used for tokenizing the reviews, and the reviews are formatted as inputs for the BERT model. The predictions are then made using the BERT model and the predicted ratings are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: The food was absolutely wonderful, from preparation to presentation, very pleasing.\n",
      "Predicted rating (BERT): 5\n",
      "---\n",
      "Review: The staff did not give us good service.\n",
      "Predicted rating (BERT): 5\n",
      "---\n",
      "Review: The restaurant was not clean. Our food was terrible.\n",
      "Predicted rating (BERT): 5\n",
      "---\n",
      "Review: The food was delicious and the service was great!\n",
      "Predicted rating (BERT): 5\n",
      "---\n",
      "Review: The food was ok. But we liked the service.\n",
      "Predicted rating (BERT): 5\n",
      "---\n",
      "Review: We ate not fine, but the food was not great at all!\n",
      "Predicted rating (BERT): 5\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Test Reviews\n",
    "test_reviews = [\n",
    "    'The food was absolutely wonderful, from preparation to presentation, very pleasing.',\n",
    "    'The staff did not give us good service.',\n",
    "    'The restaurant was not clean. Our food was terrible.',\n",
    "    'The food was delicious and the service was great!',\n",
    "    'The food was ok. But we liked the service.',\n",
    "    'We ate not fine, but the food was not great at all!'\n",
    "]\n",
    "\n",
    "# For Bert: Tokenize and format the sentences as model inputs\n",
    "inputs = tokenizer(test_reviews, return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Make predictions on the test reviews\n",
    "predictions_bert = model_bert(inputs)\n",
    "\n",
    "# Print the predictions\n",
    "for i, review in enumerate(test_reviews):\n",
    "    print(f'Review: {review}')\n",
    "    print(f'Predicted rating (BERT): {np.argmax(predictions_bert.logits[i]) + 1}')\n",
    "    print('---')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
