{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "To evaluate the performance of the trained models, we can compute the classification reports which contain the precision, recall, and F1-score for each class. We can also compute the accuracy of the models on the test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first we need to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunk 0\n",
      "Loading chunk 1\n",
      "Loading chunk 2\n",
      "Loading chunk 3\n",
      "Loading chunk 4\n",
      "Found 6468 unique tokens.\n",
      "Shape of data tensor: (1000, 250)\n",
      "Shape of label tensor: (1000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to store the preprocessed data\n",
    "df_preprocessed = pd.DataFrame()\n",
    "\n",
    "# Load the preprocessed data chunks into the DataFrame\n",
    "# The number of chunks may vary depending on the chunk size in the previous step\n",
    "\n",
    "for i in range(5): # for 5k reviews\n",
    "# for i in range(10): # for 100k reviews\n",
    "# for i in range(20): # for 1M reviews\n",
    "# for i in range(70): # for the entire dataset\n",
    "    print(f'Loading chunk {i}')\n",
    "    # Load the chunk\n",
    "    chunk = pd.read_json(f'preprocessing/preprocessed_reviews_chunk_{i}.json')\n",
    "\n",
    "    # Append the chunk to the DataFrame\n",
    "    df_bert = pd.concat([df_preprocessed, chunk])\n",
    "    df_cnn = df_bert.copy()\n",
    "    df_lstm = df_bert.copy()\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 10000\n",
    "# Max number of words in each review.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Extract the strings from the dictionaries in the 'text' column\n",
    "df_cnn['text'] = df_cnn['text'].apply(lambda x: list(x.values())[0] if isinstance(x, dict) else x)\n",
    "\n",
    "# Extract the ratings from the dictionaries in the 'stars' column\n",
    "df_cnn['stars'] = df_cnn['stars'].apply(lambda x: list(x.values())[0] if isinstance(x, dict) else x)\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@\\[\\]^_`{|}~', lower=True)\n",
    "\n",
    "# Fit the tokenizer on the texts\n",
    "tokenizer.fit_on_texts(df_cnn['text'].values)\n",
    "\n",
    "# Vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Transform text to sequence of integers\n",
    "X = tokenizer.texts_to_sequences(df_cnn['text'].values)\n",
    "\n",
    "# Pad sequences\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "# One-hot encode labels\n",
    "Y = pd.get_dummies(df_cnn['stars']).values\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 5ms/step\n",
      "Classification Report for CNN: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        10\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.52      1.00      0.68        52\n",
      "\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.10      0.20      0.14       100\n",
      "weighted avg       0.27      0.52      0.36       100\n",
      "\n",
      "Accuracy of CNN:  0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dinopelesevic/Documents/ML2/SentimentAnalysisYelpReviews/SentimentAnalysisYelpReview/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/dinopelesevic/Documents/ML2/SentimentAnalysisYelpReviews/SentimentAnalysisYelpReview/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/dinopelesevic/Documents/ML2/SentimentAnalysisYelpReviews/SentimentAnalysisYelpReview/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Load the trained CNN model\n",
    "model_cnn = load_model('models/sentiment_analysis_model_cnn_5k.h5')\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred_cnn = model_cnn.predict(X_test)\n",
    "\n",
    "# Convert the predictions from categorical to label encoded\n",
    "y_pred_cnn = np.argmax(y_pred_cnn, axis=1)\n",
    "\n",
    "# Convert the true labels from categorical to label encoded\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Compute the classification report for the CNN model\n",
    "classification_report_cnn = classification_report(y_true, y_pred_cnn)\n",
    "print(\"Classification Report for CNN: \\n\", classification_report_cnn)\n",
    "\n",
    "# Compute the accuracy of the CNN model\n",
    "accuracy_cnn = accuracy_score(y_true, y_pred_cnn)\n",
    "print(\"Accuracy of CNN: \", accuracy_cnn)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 15:53:47.871661: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-11 15:53:47.875187: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-11 15:53:47.877819: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4 [======>.......................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 15:53:48.237243: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-11 15:53:48.238361: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-11 15:53:48.240187: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 38ms/step\n",
      "Classification Report for LSTM: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        10\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.52      1.00      0.68        52\n",
      "\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.10      0.20      0.14       100\n",
      "weighted avg       0.27      0.52      0.36       100\n",
      "\n",
      "Accuracy of LSTM:  0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dinopelesevic/Documents/ML2/SentimentAnalysisYelpReviews/SentimentAnalysisYelpReview/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/dinopelesevic/Documents/ML2/SentimentAnalysisYelpReviews/SentimentAnalysisYelpReview/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/dinopelesevic/Documents/ML2/SentimentAnalysisYelpReviews/SentimentAnalysisYelpReview/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Load the trained LSTM model\n",
    "model_lstm = load_model('models/sentiment_analysis_model_lstm_5k.h5')\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred_lstm = model_lstm.predict(X_test)\n",
    "\n",
    "# Convert the predictions from categorical to label encoded\n",
    "y_pred_lstm = np.argmax(y_pred_lstm, axis=1)\n",
    "\n",
    "# Compute the classification report for the LSTM model\n",
    "classification_report_lstm = classification_report(y_true, y_pred_lstm)\n",
    "print(\"Classification Report for LSTM: \\n\", classification_report_lstm)\n",
    "\n",
    "# Compute the accuracy of the LSTM model\n",
    "accuracy_lstm = accuracy_score(y_true, y_pred_lstm)\n",
    "print(\"Accuracy of LSTM: \", accuracy_lstm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at models/sentiment_analysis_model_bert_100k were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at models/sentiment_analysis_model_bert_100k and are newly initialized: ['dropout_99']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-06-11 15:41:20.615860: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int32 and shape [1000]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 292s 5s/step\n",
      "Classification Report for BERT: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       114\n",
      "           2       0.00      0.00      0.00        72\n",
      "           3       0.15      0.59      0.24       112\n",
      "           4       0.17      0.36      0.23       256\n",
      "           5       0.00      0.00      0.00       446\n",
      "\n",
      "    accuracy                           0.16      1000\n",
      "   macro avg       0.06      0.19      0.09      1000\n",
      "weighted avg       0.06      0.16      0.08      1000\n",
      "\n",
      "Accuracy of BERT:  0.158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dinopelesevic/Documents/ML2/SentimentAnalysisYelpReviews/SentimentAnalysisYelpReview/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/dinopelesevic/Documents/ML2/SentimentAnalysisYelpReviews/SentimentAnalysisYelpReview/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/dinopelesevic/Documents/ML2/SentimentAnalysisYelpReviews/SentimentAnalysisYelpReview/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Load Libraries\n",
    "from transformers import AutoTokenizer, TFDistilBertForSequenceClassification\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model_bert = TFDistilBertForSequenceClassification.from_pretrained('models/sentiment_analysis_model_bert_5k')\n",
    "\n",
    "# Tokenize the test set\n",
    "test_encodings = tokenizer_bert(df_bert['text'].to_list(), truncation=True, padding=True)\n",
    "\n",
    "# Convert the tokenized test set to a TensorFlow dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "dict(test_encodings),\n",
    "df_bert['stars'].to_list()\n",
    "))\n",
    "\n",
    "# Batch the test set\n",
    "test_dataset = test_dataset.batch(16)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred_bert = model_bert.predict(test_dataset)\n",
    "\n",
    "# Convert the predictions from categorical to label encoded\n",
    "y_pred_bert = np.argmax(y_pred_bert.logits, axis=1)\n",
    "\n",
    "# Compute the classification report for the BERT model\n",
    "classification_report_bert = classification_report(df_bert['stars'].to_list(), y_pred_bert)\n",
    "print(\"Classification Report for BERT: \\n\", classification_report_bert)\n",
    "\n",
    "# Compute the accuracy of the BERT model\n",
    "accuracy_bert = accuracy_score(df_bert['stars'].to_list(), y_pred_bert)\n",
    "print(\"Accuracy of BERT: \", accuracy_bert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "We can compare the performance of the three models by looking at their accuracy and F1-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of CNN: 0.52, LSTM: 0.52, BERT: 0.158\n",
      "\n",
      "Classification report for CNN:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        10\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.52      1.00      0.68        52\n",
      "\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.10      0.20      0.14       100\n",
      "weighted avg       0.27      0.52      0.36       100\n",
      "\n",
      "\n",
      "Classification report for LSTM:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        10\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.52      1.00      0.68        52\n",
      "\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.10      0.20      0.14       100\n",
      "weighted avg       0.27      0.52      0.36       100\n",
      "\n",
      "\n",
      "Classification report for BERT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       114\n",
      "           2       0.00      0.00      0.00        72\n",
      "           3       0.15      0.59      0.24       112\n",
      "           4       0.17      0.36      0.23       256\n",
      "           5       0.00      0.00      0.00       446\n",
      "\n",
      "    accuracy                           0.16      1000\n",
      "   macro avg       0.06      0.19      0.09      1000\n",
      "weighted avg       0.06      0.16      0.08      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compare the models\n",
    "print(f\"Accuracy of CNN: {accuracy_cnn}, LSTM: {accuracy_lstm}, BERT: {accuracy_bert}\")\n",
    "print(\"\\nClassification report for CNN:\\n\", classification_report_cnn)\n",
    "print(\"\\nClassification report for LSTM:\\n\", classification_report_lstm)\n",
    "print(\"\\nClassification report for BERT:\\n\", classification_report_bert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
